import boto3
import os
import connectDatabase
import hashlib
import time
from dotenv import load_dotenv

os.chdir(os.path.dirname(os.path.abspath(__file__))) #ê²½ë¡œ ìµœì†Œí™” ì‹œ í•„ìš”

# .env íŒŒì¼ ë¡œë“œ (AWS ìê²© ì¦ëª… ë¶ˆëŸ¬ì˜¤ê¸°)
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ìê²© ì¦ëª… ê°€ì ¸ì˜¤ê¸°
# S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
s3 = boto3.client(
    's3',
    aws_access_key_id = os.getenv("AWS_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    region_name=os.getenv("REGION_NAME")
)


prefix = ''  # ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •í•˜ë©´ ë²„í‚· ì „ì²´ì—ì„œ ê°ì²´ë¥¼ ë‚˜ì—´í•¨

# S3ì—ì„œ í•´ë‹¹ prefix ì•„ë˜ì˜ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°
def dataDownload(root, url, bucket_name):
    paginator = s3.get_paginator('list_objects_v2')
    total_files = 0
    total_duration = 0

    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.endswith('/'):  # ë””ë ‰í„°ë¦¬ë¼ë©´ ìƒëµ
                continue

            total_files += 1
            loop_start = time.time()

            # ë¡œì»¬ ì €ì¥ ê²½ë¡œ êµ¬ì„±
            local_path = os.path.join(f'{root}/{bucket_name}', key)
            os.makedirs(os.path.dirname(local_path), exist_ok=True)

            print(f'Downloading s3://{bucket_name}/{key} -> {local_path}')
            try:
                s3.download_file(bucket_name, key, local_path)
                fileHash = get_file_hash(local_path)
                connectDatabase.updateFileHash(f'{url}/{key}', fileHash)
            except Exception as e:
                print('ì—ëŸ¬ë°œìƒ', e)

            loop_end = time.time()
            duration = loop_end - loop_start
            total_duration += duration
            print(f'â±ï¸ ë‹¤ìš´ë¡œë“œ ì‹œê°„: {duration:.2f}ì´ˆ')

    # ìµœì¢… í†µê³„ ì¶œë ¥
    if total_files > 0:
        avg_time = total_duration / total_files
    else:
        avg_time = 0

    print(f'\nğŸ“Š ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ì´ {total_files}ê°œ íŒŒì¼')
    print(f'â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_duration:.2f}ì´ˆ')
    print(f'ğŸ“ˆ í‰ê·  íŒŒì¼ë‹¹ ì‹œê°„: {avg_time:.2f}ì´ˆ')


    print(f"âœ… All files downloaded from bucket: {bucket_name}")

def get_file_hash(file_path):
    sha256_hash = hashlib.sha256()
    with open(file_path, 'rb') as f:
        for byte_block in iter(lambda: f.read(4096), b''):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()


def main(root):
    bucket_urls =connectDatabase.getDistinctBucketUrl()
    for url_tuple in bucket_urls:
        url = url_tuple[0]
        bucket_name = url.split('//')[1].split('/')[0].split('.')[0]
        if bucket_name:
            print(f"ğŸ“¦ Processing bucket: {bucket_name}")
            dataDownload(root, url, bucket_name)
            
        else:
            print(f"âŒ Invalid bucket URL: {url}")
